{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_3_3_Aumento_de_datos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNvN2UUMRYRlun7/BRp45M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_3_Aumento_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aumento de datos con TorchIO"
      ],
      "metadata": {
        "id": "ZLO3DRhOyfXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acceso al conjunto de datos de prueba\n"
      ],
      "metadata": {
        "id": "DUwE8U5JykrL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWl6EINuyWMb"
      },
      "outputs": [],
      "source": [
        "class MedicalDecathlonDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, task, google_id, batch_size, train_val_ratio):\n",
        "        super().__init__()\n",
        "        self.task = task\n",
        "        self.google_id = google_id\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset_dir = Path(task)\n",
        "        self.train_val_ratio = train_val_ratio\n",
        "        self.subjects = None\n",
        "        self.test_subjects = None\n",
        "        self.preprocess = None\n",
        "        self.transform = None\n",
        "        self.train_set = None\n",
        "        self.val_set = None\n",
        "        self.test_set = None\n",
        "    \n",
        "    def get_max_shape(self, subjects):\n",
        "        import numpy as np\n",
        "        dataset = tio.SubjectsDataset(subjects)\n",
        "        shapes = np.array([s.spatial_shape for s in dataset])\n",
        "        return shapes.max(axis=0)\n",
        "    \n",
        "    def download_data(self):\n",
        "        if not self.dataset_dir.is_dir():\n",
        "            url = f'https://drive.google.com/uc?id={self.google_id}'\n",
        "            output = f'{self.task}.tar'\n",
        "            gdown.download(url, output, quiet=False)\n",
        "            !tar xf {output}\n",
        "\n",
        "        def get_niis(d):\n",
        "            return sorted(p for p in d.glob('*.nii*') if not p.name.startswith('.'))\n",
        "\n",
        "        image_training_paths = get_niis(self.dataset_dir / 'imagesTr')\n",
        "        label_training_paths = get_niis(self.dataset_dir / 'labelsTr')\n",
        "        image_test_paths = get_niis(self.dataset_dir / 'imagesTs')\n",
        "        return image_training_paths, label_training_paths, image_test_paths\n",
        "\n",
        "    def prepare_data(self):\n",
        "        image_training_paths, label_training_paths, image_test_paths = self.download_data()\n",
        "\n",
        "        self.subjects = []\n",
        "        for image_path, label_path in zip(image_training_paths, label_training_paths):\n",
        "            # 'image' and 'label' are arbitrary names for the images\n",
        "            subject = tio.Subject(\n",
        "                image=tio.ScalarImage(image_path),\n",
        "                label=tio.LabelMap(label_path)\n",
        "            )\n",
        "            self.subjects.append(subject)\n",
        "        \n",
        "        self.test_subjects = []\n",
        "        for image_path in image_test_paths:\n",
        "            subject = tio.Subject(image=tio.ScalarImage(image_path))\n",
        "            self.test_subjects.append(subject)\n",
        "    \n",
        "    def get_preprocessing_transform(self):\n",
        "        preprocess = tio.Compose([\n",
        "            tio.RescaleIntensity((-1, 1)),\n",
        "            tio.CropOrPad(self.get_max_shape(self.subjects + self.test_subjects)),\n",
        "            tio.EnsureShapeMultiple(8),  # for the U-Net\n",
        "            tio.OneHot(),\n",
        "        ])\n",
        "        return preprocess\n",
        "    \n",
        "    def get_augmentation_transform(self):\n",
        "        augment = tio.Compose([\n",
        "            tio.RandomAffine(),\n",
        "            tio.RandomGamma(p=0.5),\n",
        "            tio.RandomNoise(p=0.5),\n",
        "            tio.RandomMotion(p=0.1),\n",
        "            tio.RandomBiasField(p=0.25),\n",
        "        ])\n",
        "        return augment\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        num_subjects = len(self.subjects)\n",
        "        num_train_subjects = int(round(num_subjects * self.train_val_ratio))\n",
        "        num_val_subjects = num_subjects - num_train_subjects\n",
        "        splits = num_train_subjects, num_val_subjects\n",
        "        train_subjects, val_subjects = random_split(self.subjects, splits)\n",
        "\n",
        "        self.preprocess = self.get_preprocessing_transform()\n",
        "        augment = self.get_augmentation_transform()\n",
        "        self.transform = tio.Compose([self.preprocess, augment])\n",
        "\n",
        "        self.train_set = tio.SubjectsDataset(train_subjects, transform=self.transform)\n",
        "        self.val_set = tio.SubjectsDataset(val_subjects, transform=self.preprocess)\n",
        "        self.test_set = tio.SubjectsDataset(self.test_subjects, transform=self.preprocess)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_set, self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_set, self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_set, self.batch_size)"
      ]
    }
  ]
}